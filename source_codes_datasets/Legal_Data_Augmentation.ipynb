{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcH2K6_vKHiq",
        "outputId": "19a77dbf-faa3-403d-e2e5-22c955a889d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset size: 105 rows\n",
            "Extracted 6 low risk words\n",
            "Extracted 24 medium risk words\n",
            "Extracted 20 high risk words\n",
            "Generating 19895 synthetic records...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19895/19895 [00:41<00:00, 480.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dataset size: 20000 rows\n",
            "Augmented dataset saved as 'LegalRiskDataset_Augmented.xlsx'\n",
            "\n",
            "Sample of synthetic data:\n",
            "                                           Paragraph  \\\n",
            "0  This offer letter outlines the conditions unde...   \n",
            "1  All parties involved , including the employer ...   \n",
            "\n",
            "                                           RiskLevel  \n",
            "0  {'letter': 'low', 'user': 'low', 'employer': '...  \n",
            "1  {'letter': 'low', 'fine': 'high', 'conditions'...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    # Also check for punkt_tab\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    # Download both punkt and punkt_tab if either is missing\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    # Try reading as Excel file first\n",
        "    df = pd.read_excel(\"/content/LegalRiskDataset.xlsx\")\n",
        "except:\n",
        "    # If that fails, try CSV\n",
        "    df = pd.read_csv(\"/content/LegalRiskDataset.xlsx\", on_bad_lines='skip')\n",
        "\n",
        "print(f'Original dataset size: {df.shape[0]} rows')\n",
        "\n",
        "# Convert string representations of dictionaries to actual dictionaries if needed\n",
        "def convert_risk_level(risk_str):\n",
        "    if isinstance(risk_str, str):\n",
        "        try:\n",
        "            return json.loads(risk_str.replace(\"'\", \"\\\"\"))\n",
        "        except:\n",
        "            # If it already looks like a dict representation but json.loads fails\n",
        "            risk_dict = {}\n",
        "            pairs = risk_str.strip('{}').split(', ')\n",
        "            for pair in pairs:\n",
        "                if ': ' in pair:\n",
        "                    key, value = pair.split(': ')\n",
        "                    key = key.strip(\"'\")\n",
        "                    value = value.strip(\"'\")\n",
        "                    risk_dict[key] = value\n",
        "            return risk_dict\n",
        "    return risk_str  # Already a dictionary or other format\n",
        "\n",
        "df['RiskLevel'] = df['RiskLevel'].apply(convert_risk_level)\n",
        "\n",
        "# Extract and analyze the original data\n",
        "paragraphs = df['Paragraph'].tolist()\n",
        "risk_levels = df['RiskLevel'].tolist()\n",
        "\n",
        "# Create word bank for each risk level\n",
        "low_risk_words = set()\n",
        "medium_risk_words = set()\n",
        "high_risk_words = set()\n",
        "\n",
        "for risk_dict in risk_levels:\n",
        "    if isinstance(risk_dict, dict):\n",
        "        for word, level in risk_dict.items():\n",
        "            if level.lower() == 'low':\n",
        "                low_risk_words.add(word.lower())\n",
        "            elif level.lower() == 'medium':\n",
        "                medium_risk_words.add(word.lower())\n",
        "            elif level.lower() == 'high':\n",
        "                high_risk_words.add(word.lower())\n",
        "\n",
        "print(f\"Extracted {len(low_risk_words)} low risk words\")\n",
        "print(f\"Extracted {len(medium_risk_words)} medium risk words\")\n",
        "print(f\"Extracted {len(high_risk_words)} high risk words\")\n",
        "\n",
        "# Extract common legal phrases from the paragraphs\n",
        "legal_phrases = []\n",
        "for para in paragraphs:\n",
        "    words = word_tokenize(para)\n",
        "    phrases = [' '.join(words[i:i+3]) for i in range(len(words)-2)]\n",
        "    legal_phrases.extend(phrases)\n",
        "\n",
        "# Function to generate a synthetic paragraph with known risk words\n",
        "def generate_synthetic_paragraph(original_paragraphs, phrases, low_words, medium_words, high_words, min_length=50, max_length=200):\n",
        "    # Start with a randomly selected paragraph as base\n",
        "    base = random.choice(original_paragraphs)\n",
        "    words = word_tokenize(base)\n",
        "\n",
        "    # Determine target length\n",
        "    target_length = random.randint(min_length, max_length)\n",
        "\n",
        "    # If base is too long, truncate\n",
        "    if len(words) > target_length:\n",
        "        words = words[:target_length]\n",
        "\n",
        "    # Create a pool of known risk words\n",
        "    all_risk_words = list(low_words) + list(medium_words) + list(high_words)\n",
        "\n",
        "    # If base is too short, add phrases and ensure we include some known risk words\n",
        "    while len(words) < target_length:\n",
        "        # Every so often, inject a known risk word\n",
        "        if random.random() < 0.3 and all_risk_words:  # 30% chance to insert a risk word\n",
        "            words.append(random.choice(all_risk_words))\n",
        "        else:\n",
        "            phrase = random.choice(phrases).split()\n",
        "            words.extend(phrase)\n",
        "\n",
        "    # Final trimming to target length\n",
        "    words = words[:target_length]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Function to generate synthetic risk annotations - MODIFIED\n",
        "def generate_synthetic_risk(paragraph, low_words, medium_words, high_words, min_words=5, max_words=8):\n",
        "    words = word_tokenize(paragraph.lower())\n",
        "\n",
        "    # Create risk dictionary only with words that have known risk levels\n",
        "    risk_dict = {}\n",
        "\n",
        "    # Check for low risk words in paragraph\n",
        "    for word in set(words):\n",
        "        if word in low_words:\n",
        "            risk_dict[word] = 'low'\n",
        "        elif word in medium_words:\n",
        "            risk_dict[word] = 'medium'\n",
        "        elif word in high_words:\n",
        "            risk_dict[word] = 'high'\n",
        "\n",
        "    # If we have too many risk words, randomly select a subset\n",
        "    if len(risk_dict) > max_words:\n",
        "        keys_to_keep = random.sample(list(risk_dict.keys()), max_words)\n",
        "        risk_dict = {k: risk_dict[k] for k in keys_to_keep}\n",
        "\n",
        "    # If we have too few risk words, we'll need to generate a new paragraph\n",
        "    # The calling code will handle this case\n",
        "\n",
        "    return risk_dict\n",
        "\n",
        "# Generate synthetic data\n",
        "target_size = 20000\n",
        "num_synthetic = target_size - len(df)\n",
        "\n",
        "print(f\"Generating {num_synthetic} synthetic records...\")\n",
        "\n",
        "synthetic_paragraphs = []\n",
        "synthetic_risks = []\n",
        "\n",
        "for _ in tqdm(range(num_synthetic)):\n",
        "    # Generate paragraph with known risk words\n",
        "    paragraph = generate_synthetic_paragraph(paragraphs, legal_phrases, low_risk_words, medium_risk_words, high_risk_words)\n",
        "\n",
        "    # Generate risk annotation (only for known risk words)\n",
        "    risk = generate_synthetic_risk(paragraph, low_risk_words, medium_risk_words, high_risk_words)\n",
        "\n",
        "    # Ensure we have at least a minimum number of risk annotations\n",
        "    attempts = 0\n",
        "    while len(risk) < 5 and attempts < 10:  # Require at least 2 risk words, try up to 5 times\n",
        "        paragraph = generate_synthetic_paragraph(paragraphs, legal_phrases, low_risk_words, medium_risk_words, high_risk_words)\n",
        "        risk = generate_synthetic_risk(paragraph, low_risk_words, medium_risk_words, high_risk_words)\n",
        "        attempts += 1\n",
        "\n",
        "    # Only add if we have sufficient risk annotations\n",
        "    if len(risk) >= 5:\n",
        "        synthetic_paragraphs.append(paragraph)\n",
        "        synthetic_risks.append(risk)\n",
        "    else:\n",
        "        # If we couldn't generate a good paragraph after 5 attempts, add one anyway\n",
        "        # but make sure it has at least one risk word\n",
        "        if len(risk) > 2:\n",
        "            synthetic_paragraphs.append(paragraph)\n",
        "            synthetic_risks.append(risk)\n",
        "\n",
        "# Create DataFrame with synthetic data\n",
        "synthetic_df = pd.DataFrame({\n",
        "    'Paragraph': synthetic_paragraphs,\n",
        "    'RiskLevel': synthetic_risks\n",
        "})\n",
        "\n",
        "# If we didn't get enough samples, generate more\n",
        "if len(synthetic_paragraphs) < num_synthetic:\n",
        "    print(f\"Generated only {len(synthetic_paragraphs)} valid samples. Continuing...\")\n",
        "\n",
        "# Combine original and synthetic data\n",
        "augmented_df = pd.concat([df, synthetic_df], ignore_index=True)\n",
        "\n",
        "print(f\"Final dataset size: {augmented_df.shape[0]} rows\")\n",
        "\n",
        "# Save the augmented dataset\n",
        "augmented_df.to_excel(\"LegalRiskDataset_Augmented.xlsx\", index=False)\n",
        "print(\"Augmented dataset saved as 'LegalRiskDataset_Augmented.xlsx'\")\n",
        "\n",
        "# Display a sample of synthetic data\n",
        "print(\"\\nSample of synthetic data:\")\n",
        "print(synthetic_df.head(2))"
      ]
    }
  ]
}